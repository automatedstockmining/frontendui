{"ast":null,"code":"import _regeneratorRuntime from\"/Users/jamesmacquillan/Documents/chatbot-im-1o/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";import _asyncToGenerator from\"/Users/jamesmacquillan/Documents/chatbot-im-1o/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";import{Configuration,OpenAIApi}from'openai';export var davinci=/*#__PURE__*/function(){var _ref=_asyncToGenerator(/*#__PURE__*/_regeneratorRuntime().mark(function _callee(prompt,key){var configuration,openai,response;return _regeneratorRuntime().wrap(function _callee$(_context){while(1)switch(_context.prev=_context.next){case 0:configuration=new Configuration({apiKey:key});openai=new OpenAIApi(configuration);_context.next=4;return openai.createChatCompletion({model:'gpt-3.5-turbo',messages:[{role:'system',content:\"you're an a AI assistant that replies to all my questions in markdown format.\"},{role:'user',content:'hi'},{role:'assistant',content:'Hi! How can I help you?'},{role:'user',content:\"\".concat(prompt,\"?\")}],temperature:0.3,max_tokens:1000,top_p:0.3,frequency_penalty:0.5,presence_penalty:0.2});case 4:response=_context.sent;return _context.abrupt(\"return\",response);case 6:case\"end\":return _context.stop();}},_callee);}));return function davinci(_x,_x2){return _ref.apply(this,arguments);};}();","map":{"version":3,"names":["Configuration","OpenAIApi","davinci","_ref","_asyncToGenerator","_regeneratorRuntime","mark","_callee","prompt","key","configuration","openai","response","wrap","_callee$","_context","prev","next","apiKey","createChatCompletion","model","messages","role","content","concat","temperature","max_tokens","top_p","frequency_penalty","presence_penalty","sent","abrupt","stop","_x","_x2","apply","arguments"],"sources":["/Users/jamesmacquillan/Documents/chatbot-im-1o/src/utils/davinci.js"],"sourcesContent":["import { Configuration, OpenAIApi } from 'openai';\n\nexport const davinci = async (prompt, key) => {\n  const configuration = new Configuration({\n    apiKey: key,\n  });\n\n  const openai = new OpenAIApi(configuration);\n\n  const response = await openai.createChatCompletion({\n    model: 'gpt-3.5-turbo',\n    messages: [\n      {\n        role: 'system',\n        content:\n          \"you're an a AI assistant that replies to all my questions in markdown format.\",\n      },\n      { role: 'user', content: 'hi' },\n      { role: 'assistant', content: 'Hi! How can I help you?' },\n      { role: 'user', content: `${prompt}?` },\n    ],\n    temperature: 0.3,\n    max_tokens: 1000,\n    top_p: 0.3,\n    frequency_penalty: 0.5,\n    presence_penalty: 0.2,\n  });\n\n  return response;\n};\n"],"mappings":"wRAAA,OAASA,aAAa,CAAEC,SAAS,KAAQ,QAAQ,CAEjD,MAAO,IAAM,CAAAC,OAAO,6BAAAC,IAAA,CAAAC,iBAAA,cAAAC,mBAAA,GAAAC,IAAA,CAAG,SAAAC,QAAOC,MAAM,CAAEC,GAAG,MAAAC,aAAA,CAAAC,MAAA,CAAAC,QAAA,QAAAP,mBAAA,GAAAQ,IAAA,UAAAC,SAAAC,QAAA,iBAAAA,QAAA,CAAAC,IAAA,CAAAD,QAAA,CAAAE,IAAA,SACjCP,aAAa,CAAG,GAAI,CAAAV,aAAa,CAAC,CACtCkB,MAAM,CAAET,GACV,CAAC,CAAC,CAEIE,MAAM,CAAG,GAAI,CAAAV,SAAS,CAACS,aAAa,CAAC,CAAAK,QAAA,CAAAE,IAAA,SAEpB,CAAAN,MAAM,CAACQ,oBAAoB,CAAC,CACjDC,KAAK,CAAE,eAAe,CACtBC,QAAQ,CAAE,CACR,CACEC,IAAI,CAAE,QAAQ,CACdC,OAAO,CACL,+EACJ,CAAC,CACD,CAAED,IAAI,CAAE,MAAM,CAAEC,OAAO,CAAE,IAAK,CAAC,CAC/B,CAAED,IAAI,CAAE,WAAW,CAAEC,OAAO,CAAE,yBAA0B,CAAC,CACzD,CAAED,IAAI,CAAE,MAAM,CAAEC,OAAO,IAAAC,MAAA,CAAKhB,MAAM,KAAI,CAAC,CACxC,CACDiB,WAAW,CAAE,GAAG,CAChBC,UAAU,CAAE,IAAI,CAChBC,KAAK,CAAE,GAAG,CACVC,iBAAiB,CAAE,GAAG,CACtBC,gBAAgB,CAAE,GACpB,CAAC,CAAC,QAjBIjB,QAAQ,CAAAG,QAAA,CAAAe,IAAA,QAAAf,QAAA,CAAAgB,MAAA,UAmBPnB,QAAQ,0BAAAG,QAAA,CAAAiB,IAAA,MAAAzB,OAAA,GAChB,kBA3BY,CAAAL,OAAOA,CAAA+B,EAAA,CAAAC,GAAA,SAAA/B,IAAA,CAAAgC,KAAA,MAAAC,SAAA,OA2BnB"},"metadata":{},"sourceType":"module","externalDependencies":[]}